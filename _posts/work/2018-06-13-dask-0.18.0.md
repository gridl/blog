---
layout: post
title: Dask Release 0.18.0
category: work
tags: [Programming, Python, scipy, dask]
theme: twitter
---
{% include JB/setup %}

*This work is supported by [Anaconda Inc.](http://anaconda.com)*

I'm pleased to announce the release of Dask version 0.18.0.  This is a major
release with breaking changes and new features.
The last release was 0.17.5 on May 4th.
This blogpost outlines notable changes since the the last release blogpost for
0.17.2 on March 21st.

You can conda install Dask:

    conda install dask

or pip install from PyPI:

    pip install dask[complete] --upgrade

Full changelogs are available here:

-  [dask/dask](https://github.com/dask/dask/blob/master/docs/source/changelog.rst)
-  [dask/distributed](https://github.com/dask/distributed/blob/master/docs/source/changelog.rst)

We list some breaking changes below, followed up by changes that are less
important, but still fun afterwards.


## Breaking changes

1.  Replaced the get=dask.threaded.get keyword with scheduler='threading'
2.  Centralized configuration
3.  replaced dask.set_options with dask.config.set
4.  Dropped dask.array.learn
5.  name= in map_blocks
6.  worker_client automatically rejoins
7.  dask.distributed protocol now interprets msgpack arrays as tuples rather
    than lists

### Replaced the get=dask.threaded.get keyword with scheduler='threading'

Dask can execute code with a variety of scheduler backends based on threads,
processes, single-threaded execution, or distributed clusters.

Previously, users selected between these backends using the somewhat
generically named `get=` keyword:

```python
x.compute(get=dask.threaded.get)
x.compute(get=dask.multiprocessing.get)
x.compute(get=dask.local.get_sync)
```

We've replaced this with a newer, and hopefully more clear, `scheduler=` keyword:

```python
x.compute(scheduler='threads')
x.compute(scheduler='processes')
x.compute(scheduler='single-threaded')
```

The `get=` keyword has been deprecated, and will go away entirely on the next major release.

For more information, see [documentation on selecting different schedulers](http://dask.pydata.org/en/latest/scheduling.html)

### Centralized configuration

Taking full advantage of Dask sometimes requires user configuration. This might
be to control logging verbosity, specify cluster configuration, provide
credentials for security, or any of several other options that arise in
production.

We've found that different computing cultures like to specify configuration in
several different ways:

1.  Configuration files
2.  Environment variables
3.  Directly within Python code

Previously this was handled with a variety of different solutions among the
different dask-foo subprojects.  Dask.distributed had one system,
dask-kubernetes had another, and so on.

Now we centralize configuration in the `dask.config` module, which collects
configuration from config files, environment variables, and runtime code, and
makes it centrally available to all Dask subprojects.  A number of Dask
subprojects (dask.distributed,
[dask-kubernetes](http://dask-kubernetes.readthedocs.io/en/latest/), and
[dask-jobqueue](http://dask-jobqueue.readthedocs.io/en/latest/), are being
co-released at the same time to take advantage of this.

If you were actively using Dask.distributed's configuration files some things
have changed:

1.  The configuration is now namespaced and more heavily nested.  Here is an
    example from the standard config file today:

	```yaml
	distributed:
	  version: 2
	  scheduler:
		allowed-failures: 3     # number of retries before a task is considered bad
		bandwidth: 100000000    # 100 MB/s estimated worker-worker bandwidth
		work-stealing: True     # workers should steal tasks from each other
		worker-ttl: null        # like '60s'. Workers must heartbeat faster than this

	  worker:
		multiprocessing-method: forkserver
		use-file-locking: True
    ```
2.  The default configuration location has moved from `~/.dask/config.yaml` to
    `~/.config/dask/distributed.yaml`

However, your old configuration files will still be found and their values
will be used appropriately.  We don't make any attempt to migrate your old
config values to the new location though.  You may want to delete the
auto-generated `~/.dask/config.yaml` file at some point, if felt like being
particularly clean.

You can learn more about Dask's configuration in [Dask's configuration
documentation](http://dask.pydata.org/en/latest/configuration.html)

### Deplaced dask.set_options with dask.config.set

Related to the configuration changes, we now include runtime state in the
configuration.  Previously people used to set runtime state with the
`dask.set_options` context manager.  Now we recommend using `dask.config.set`:

```python
with dask.set_options(scheduler='threads'):  # Before
    ...

with dask.config.set(scheduler='threads'):  # After
    ...
```

The `dask.set_options` function is now an alias to `dask.config.set`.

### Removed the dask.array.learn subpackage

This saw very little use.  All functionality (and much much more) is now
available in [Dask-ML](http://dask-ml.readthedocs.io/en/latest/)


### Other

-   We've removed the `token=` keyword from map_blocks and moved the
    functionality to the `name=` keyword.
-   The `dask.distributed.worker_client` automatically rejoins the threadpool when
    you close the context manager
-   The Dask.distributed protocol now interprets msgpack arrays as tuples
    rather than lists.  As a reminder, using different versions of
    Dask.distributed workers/scheduler/clients is not supported.

## Fun changes

###  Arrays

#### Generalized Universal Functions

Dask.array now supports Numpy-style
[Generalized Universal Functions (gufuncs)](https://docs.scipy.org/doc/numpy-1.13.0/reference/c-api.generalized-ufuncs.html)
transparently.
As a reminder, Numpy's gufuncs specify the axes along which they can be easily
broadcast and the axes that they need to have fully in memory.
This means that you can apply a broader set of Numpy functions on Dask arrays
and have everything *just work*.

Here are a few examples:

1.  Apply a normal Numpy GUFunc, `eig`, directly onto a Dask array
    ```python
    import dask.array as da
    import numpy as np

    # Apply a Numpy GUFunc, eig, directly onto a Dask array
    x = da.random.normal(size=(3, 10, 10), chunks=(2, 10, 10))
    w, v = np.linalg._umath_linalg.eig(x, output_dtypes=(float, float))
    ```

    Numpy has gufuncs of many of its internal functions, but they haven't
    yet decided to switch these out to the public API.

2.  We can define our own gufunc from a Python function

    ```python
    @da.as_gufunc(signature="(i)->()", output_dtypes=float, vectorize=True)
    def gufoo(x):
        return np.mean(x, axis=-1)

    y = gufoo(x)
    ```

    Here we use just `np.mean`, but you could possibly imagine something more
    complex.

3.  We can also define GUFuncs with other projects, like Numba

    ```python
    import numba

    @numba.vectorize([float64(float64, float64)])
    def f(x, y):
        return x + y

    f(x, y)
    ```

    What I like about this is that Dask and Numba developers didn't coordinate
    at all on this feature, it's just that they both support the Numpy GUFunc
    protocol, so you get interactions like this for free.

However, there are several limitations to Dask's use of gufuncs.  Inner or core
dimensions must be single-chunked.  This is a good opportunity to use the
``da.rechunk`` method and [rechunking](http://dask.pydata.org/en/latest/array-chunks.html#rechunking).

For more information see [Dask's GUFunc documentation](http://dask.pydata.org/en/latest/array-gufunc.html).  This work was done by [Markus Gonser (@magonser)](https://github.com/magonser).


#### New "auto" value for rechunking

How you lay out your arrays into chunks can greatly affect the performance of
many algorithms.  Often expert users will explicitly rechunk their arrays
before doing certain algorithms like FFTs, matrix multiplies, covariance
computations, etc..  For example they might want to rechunk an array so that
the `0`th dimension has a single chunk and the `1`st dimension expands or
contracts as is necessary to maintain a good chunk size.  Previously they had
to do a bit of math to determine what size to make the chunks in that 1st
dimension.  Now they can write something like this:

```python
with dask.config.set({'array.chunk-size': '100MiB'}):
    x = x.rechunk({
        0: x.shape[0],  # single chunk in this dimension
        1: 'auto'       # respond as necessary in this dimension
    })
```

Auto can be used anywhere that accepts chunks.

```python
x = da.from_array(x, chunks='auto')
```

To be clear though, this doesn't do "automatic chunking", which is a very hard
problem in general.  Users still need to be aware of their computations and how
they want to chunk, this just makes it marginally easier to make good
decisions.


#### Algorithmic improvements

Dask.array also gained an [einsum](http://dask.pydata.org/en/latest/array-api.html#dask.array.einsum) implementation thanks to [Simon Perkins](https://github.com/sjperkins).

Thanks to work from [Jeremy Chan](https://github.com/convexset), Dask.array's
QR decompositions became nicer in two ways:

1.  They support [short-and-fat arrays](http://dask.pydata.org/en/latest/array-api.html#dask.array.linalg.sfqr)
2.  The [tall-and-skinny](http://dask.pydata.org/en/latest/array-api.html#dask.array.linalg.tsqr)
    variant now operates more robustly in less memory.  Here is a friendly GIF
    of execution:

    <img src="https://user-images.githubusercontent.com/306380/41350133-175cac7e-6ee0-11e8-9a0e-785c6e846409.gif" width="40%">

Native support for the [Zarr format](http://zarr.readthedocs.io/en/stable/) for
chunked n-dimensional arrays landed thanks to [Martin
Durant](https://github.com/martindurant) and [John A
Kirkham](https://github.com/jakirkham).  Zarr has been especially useful due to
its speed, simple spec, support of the full NetCDF style conventions, and
amenability to cloud storage.

### Dataframes

#### Pandas 0.23.0 support

Thanks to [Tom Augspurger](https://github.com/TomAugspurger) Dask.dataframe is
consistent with changes in the recent Pandas 0.23 release.

#### Orc support

Dask.dataframe has grown a [dd.read_orc](http://dask.pydata.org/en/latest/dataframe-api.html#dask.dataframe.read_hdf) function.  This depends on PyArrow for data access.  Thanks to [Jim Crist](https://github.com/jcrist) for the work on the Arrow side and [Martin Durant](https://github.com/martindurant) for parallelizing it with Dask.

#### Read_json support

Dask.dataframe now has a
[dd.read_json](http://dask.pydata.org/en/latest/dataframe-api.html#dask.dataframe.read_json)
function to match the `pandas.read_json` function.  This came about shortly
after a recent [PyCon 2018 talk comparing Spark and Dask
dataframe](https://www.youtube.com/watch?v=X4YHGKj3V5M) where [Irina
Truong](https://github.com/j-bennet) mentioned that it was missing.  Thanks to
[Martin Durant](https://github.com/martindurant) and [Irina
Truong](https://github.com/j-bennet) for this contribution.

See the [dataframe data ingestion documentation](http://dask.pydata.org/en/latest/dataframe-create.html)
for more information about either JSON, ORC, or any of the other formats
supported by Dask.dataframe.


### Joblib

[Joblib](https://pythonhosted.org/joblib/), the parallelism library behind most
of Scikit-Learn has had a [Dask backend](http://dask-ml.readthedocs.io/en/latest/joblib.html)
for a while now.  While it has always been pretty easy to use, it's now
becoming much easier to *use well*.  After using this in practice for a while
together with the Scikit-Learn developers, we've identified and smoothed over a
number of usability issues:

-  Simpler startup:

    Previously you had to explicitly import `distributed.joblib`, in order to
    register the Dask backend, this is no longer necessary.  Scikit-Learn will
    the find the backend automatically.

-  Using Dask globally:

    Previously you always had to wrap Joblib/Scikit-Learn commands within a
    context manager:

    ```python
    from dask.distributed import Client
    client = Client()  # start a local cluster

    from sklearn.externals import joblib

    with joblib.parallel_backend("dask"):
        # run my scikit-learn code here
    ```

    This is still the right way to do things when writing serious software, but
    the context manager was a bit of a pain for interactive use.  Now you can
    set Joblib to use Dask globally.

    ```


-   Joblib
    1.  simpler startup and global access
    2.  interactive use
    2.  keywords
    3.  reference cycle
    4.  Automatic scattering

3.  Other
    -   Improved web presence, consistent styling on rtd
    -   Serialization families, starting to make client secure to pickle


Consider thanking a couple contributors particularly

Diane Trout Debian





### Tornado 5.0

Tornado is a popular framework for concurrent network programming that Dask
relies on heavily.  Tornado recently released a major version update that
included both some major features for Dask as well as a couple of bugs.

The new `IOStream.read_into` method allows Dask communications (or anyone using
this API) to move large datasets more efficiently over the network with
fewer copies.  This enables Dask to take advantage of high performance
networking available on modern super-computers.  On the Cheyenne system, where
we tested this, we were able to get the full 3GB/s bandwidth available through
the Infiniband network with this change (when using a few worker processes).

Many thanks to [Antoine Pitrou](https://github.com/pitrou) and [Ben
Darnell](https://github.com/bdarnell) for their efforts on this.

At the same time there were some unforeseen issues in the update to Tornado 5.0.
More pervasive use of bytearrays over bytes caused issues with compression
libraries like Snappy and Python 2 that were not expecting these types.  There
is a brief window in `distributed.__version__ == 1.21.3` that enables this
functionality if Tornado 5.0 is present but will misbehave if Snappy is also
present.

### HTTP File System

Dask leverages a [file-system-like protocol](https://github.com/dask/dask/issues/2880)
for access to remote data.
This is what makes commands like the following work:

```python
import dask.dataframe as dd

df = dd.read_parquet('s3://...')
df = dd.read_parquet('hdfs://...')
df = dd.read_parquet('gcs://...')
```

We have now added http and https file systems for reading data directly from
web servers.  These also support random access if the web server supports range
queries.

```python
df = dd.read_parquet('https://...')
```

As with S3, HDFS, GCS, ... you can also use these tools outside of Dask
development.  Here we read the first twenty bytes of the Pandas license:

```python
from dask.bytes.http import HTTPFileSystem
http = HTTPFileSystem()
with http.open('https://raw.githubusercontent.com/pandas-dev/pandas/master/LICENSE') as f:
    print(f.read(20))
```

```
b'BSD 3-Clause License'
```

Thanks to [Martin Durant](https://github.com/martindurant) who did this work
and manages Dask's byte handling generally.  See [remote data documentation](http://dask.pydata.org/en/latest/remote-data-services.html) for more information.


### Fixed a correctness bug in Dask dataframe's shuffle

We identified and resolved a correctness bug in dask.dataframe's shuffle that
resulted in some rows being dropped during complex operations like joins and
groupby-applies with many partitions.

See [dask/dask #3201](https://github.com/dask/dask/pull/3201) for more information.


### Cluster super-class and intelligent adaptive deployments

There are many Python subprojects that help you deploy Dask on different
cluster resource managers like Yarn, SGE, Kubernetes, PBS, and more.  These
have all converged to have more-or-less the same API that we have now combined
into a consistent interface that downstream projects can inherit from in
`distributed.deploy.Cluster`.

Now that we have a consistent interface we have started to invest more in
improving the interface and intelligence of these systems as a group.  This
includes both pleasant IPython widgets like the following:

<img src="{{BASE_PATH}}/images/dask-kubernetes-widget.png" width="70%">

as well as improved logic around adaptive deployments.  Adaptive deployments
allow clusters to scale themselves automatically based on current workload.  If
you have recently submitted a lot of work the scheduler will estimate its
duration and ask for an appropriate number of workers to finish the computation
quickly.  When the computation has finished the scheduler will release the
workers back to the system to free up resources.

The logic here has improved substantially including the following:

-  You can specify minimum and maximum limits on your adaptivity
-  The scheduler estimates computation duration and asks for workers
   appropriately
-  There is some additional delay in giving back workers to avoid hysteresis,
   or cases where we repeatedly ask for and return workers


Related projects
----------------

Some news from related projects:

-   The young daskernetes project was renamed to [dask-kubernetes](http://dask-kubernetes.readthedocs.io/en/latest/).  This displaces a previous project (that had not been released) for launching Dask on Google Cloud Platform.  That project has been renamed to [dask-gke](https://github.com/dask/dask-gke).
-   A new project, [dask-jobqueue](https://github.com/dask/dask-jobqueue/) was
    started to handle launching Dask clusters on traditional batch queuing
    systems like PBS, SLURM, SGE, TORQUE, etc..  This projet grew out of the [Pangeo](https://pangeo-data.github.io/) collaboration
-   A Dask Helm chart has been added to Helm's stable channel


Acknowledgements
----------------

The following people contributed to the dask/dask repository since the 0.17.0
release on February 12h:

-  Anderson Banihirwe
-  Dan Collins
-  Dieter Weber
-  Gabriele Lanaro
-  John Kirkham
-  James Bourbeau
-  Julien Lhermitte
-  Matthew Rocklin
-  Martin Durant
-  Max Epstein
-  nkhadka
-  okkez
-  Pangeran Bottor
-  Rich Postelnik
-  Scott M. Edenbaum
-  Simon Perkins
-  Thrasibule
-  Tom Augspurger
-  Tor E Hagemann
-  Uwe L. Korn
-  Wes Roach

The following people contributed to the dask/distributed repository since the
1.21.0 release on February 12th:

-  Alexander Ford
-  Andy Jones
-  Antoine Pitrou
-  Brett Naul
-  Joe Hamman
-  John Kirkham
-  Loïc Estève
-  Matthew Rocklin
-  Matti Lyra
-  Sven Kreiss
-  Thrasibule
-  Tom Augspurger
